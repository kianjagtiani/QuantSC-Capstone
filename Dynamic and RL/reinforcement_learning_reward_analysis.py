# -*- coding: utf-8 -*-
"""reinforcement_learning_reward_analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DNhcgeJKB2Z-WB71fO0QvKzrglX69-YB
"""

!pip install stable-baselines3[extra] gymnasium

# if you want the classic OpenAI Gym:
!pip install gym

# or, if you prefer Gymnasium (and you haven’t yet):
!pip install gymnasium

# plus Stable-Baselines3 extras:
!pip install stable-baselines3[extra]

import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt

# ─── Dynamic import of Gymnasium vs classic Gym ───────────────────────────────
try:
    import gymnasium as gym
    from gymnasium import spaces
    USE_GYMNASIUM = True
except ImportError:
    import gym
    from gym import spaces
    USE_GYMNASIUM = False

# ─── I) Single‐step ETF pair strategy ──────────────────────────────────────────
class ETFPairTradingStrategy:
    def __init__(self, bull_weight, bear_weight, rebalance_threshold, inout_threshold):
        total = bull_weight + bear_weight
        self.bull_weight      = bull_weight / total
        self.bear_weight      = bear_weight / total
        self.rebalance_thresh = rebalance_threshold
        self.inout_thresh     = inout_threshold

    def reset(self, initial_capital):
        self.long_t        = initial_capital
        self.sh_bull       = self.bull_weight * initial_capital
        self.sh_bear       = self.bear_weight * initial_capital
        self.current_value = self.long_t + self.sh_bull + self.sh_bear
        return self.current_value

    def step(self, ret_bull, ret_bear, ret_tbill):
        prev = self.current_value
        # mark‐to‐market each leg
        self.sh_bull *= (1 - ret_bull)
        self.sh_bear *= (1 - ret_bear)
        self.long_t   *= (1 + ret_tbill)

        cost = 0.0
        ts = self.sh_bull + self.sh_bear

        # rebalance if bull‐ratio ∉ [bw−δ, bw+δ]
        curr_bw = (self.sh_bull / ts) if ts > 0 else self.bull_weight
        low, high = self.bull_weight - self.rebalance_thresh, self.bull_weight + self.rebalance_thresh
        if curr_bw < low or curr_bw > high:
            traded = 2 * abs(self.bull_weight - curr_bw) * ts
            self.sh_bull = self.bull_weight * ts
            self.sh_bear = self.bear_weight * ts
            cost += traded

        # inflow/outflow on margin breaches
        ts = self.sh_bull + self.sh_bear
        ratio = (self.long_t / ts) if ts > 0 else 1.0
        if ratio < (1 - self.inout_thresh):
            traded = abs(1 - ratio) * ts
            factor = ts / self.long_t
            self.sh_bull /= factor
            self.sh_bear /= factor
            cost += traded
        elif ratio > (1 + self.inout_thresh):
            traded = abs(ratio - 1) * ts
            factor = self.long_t / ts
            self.sh_bull *= factor
            self.sh_bear *= factor
            cost += traded

        # new portfolio value & reward
        self.current_value = self.long_t + self.sh_bull + self.sh_bear
        reward = (self.current_value - prev) - 0.001 * cost
        return self.current_value, reward

# ─── II) Gym Env wrapper ───────────────────────────────────────────────────────
class PairEnv(gym.Env):
    metadata = {"render.modes": ["human"]}

    def __init__(self, returns_df, window=20, initial_capital=10_000):
        super().__init__()
        self.returns  = returns_df.reset_index(drop=True)
        self.n        = len(self.returns)
        self.window   = window
        self.capital  = initial_capital

        # Obs: [20d SPX vol, SPX compounded return, bull‐weight ratio, margin ratio]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32
        )
        # Act: [rebalance_thresh ∈ [0,0.2], bull_weight ∈ [0,1]]
        self.action_space      = spaces.Box(
            low=np.array([0.0,0.0]), high=np.array([0.2,1.0]), dtype=np.float32
        )

    def reset(self, *, seed=None, options=None, **kwargs):
        if USE_GYMNASIUM:
            super().reset(seed=seed)
        self.t = self.window
        self.strategy = ETFPairTradingStrategy(0.5, 0.5, 0.05, 0.05)
        self.strategy.reset(self.capital)
        obs = self._get_state()
        return (obs, {}) if USE_GYMNASIUM else obs

    def step(self, action):
        δ, w = action
        self.strategy.rebalance_thresh = float(δ)
        self.strategy.bull_weight      = float(w)
        self.strategy.bear_weight      = 1.0 - float(w)

        r = self.returns.loc[self.t]
        _, reward = self.strategy.step(r["UPRO"], r["SPXU"], r["TBill"])
        self.t += 1
        done = self.t >= self.n
        obs = self._get_state()
        if USE_GYMNASIUM:
            return obs, reward, done, False, {}
        else:
            return obs, reward, done, {}

    def _get_state(self):
        spx_w = self.returns["^GSPC"].iloc[self.t-self.window:self.t]
        vol   = spx_w.std()
        mom   = np.prod(1 + spx_w.values) - 1
        bw    = self.strategy.sh_bull / (self.strategy.sh_bull + self.strategy.sh_bear)
        mr    = self.strategy.long_t   / (self.strategy.sh_bull + self.strategy.sh_bear)
        return np.array([vol, mom, bw, mr], dtype=np.float32)

# ─── III) Data prep & train PPO ────────────────────────────────────────────────
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback

# 1) Download auto‐adjusted "Close"
tickers = ["^GSPC","UPRO","SPXU"]
df = yf.download(
    tickers,
    start="2010-01-01",
    end="2020-01-01",
    auto_adjust=True,
    progress=False
)["Close"]

# 2) Compute returns & T‐bill
rets = df.pct_change().dropna()
rets["TBill"] = (1 + 0.02)**(1/252) - 1

# 3) Train/eval split (80/20 so eval_len > window)
split = int(len(rets) * 0.8)
train_df, eval_df = rets.iloc[:split], rets.iloc[split:]
train_env = PairEnv(train_df, window=20)
eval_env  = PairEnv(eval_df,  window=20)

eval_callback = EvalCallback(
    eval_env,
    best_model_save_path="./logs/",
    log_path="./logs/",
    eval_freq=500,
    n_eval_episodes=5
)

model = PPO("MlpPolicy", train_env, learning_rate=3e-4, batch_size=64, verbose=1)
model.learn(total_timesteps=50_000, callback=eval_callback)
model.save("ppo-pair-agent")

# ─── IV) Backtest & plot ───────────────────────────────────────────────────────
model = PPO.load("./logs/best_model.zip")

if USE_GYMNASIUM:
    obs, _ = eval_env.reset()
else:
    obs = eval_env.reset()

port_vals = [eval_env.strategy.current_value]
done = False
while not done:
    action, _ = model.predict(obs, deterministic=True)
    if USE_GYMNASIUM:
        obs, _, done, _, _ = eval_env.step(action)
    else:
        obs, _, done, _    = eval_env.step(action)
    port_vals.append(eval_env.strategy.current_value)

# align lengths by dropping initial seed and slicing the index to match
pv = pd.Series(port_vals[1:], index=eval_df.index[20:])
bh = 10_000 * (1 + eval_df["^GSPC"].iloc[20:]).cumprod()

plt.figure(figsize=(12,6))
plt.plot(pv, label="RL-Pair Strategy")
plt.plot(bh, label="S&P 500 B&H")
plt.title("Out‐of‐Sample Backtest (Portfolio Value)")
plt.xlabel("Date")
plt.ylabel("Portfolio Value ($)")
plt.legend()
plt.grid(True)
plt.show()

import subprocess
import sys

# ─── Ensure stable-baselines3 is installed ────────────────────────────────────
subprocess.check_call([sys.executable, "-m", "pip", "install", "stable-baselines3[extra]"])

import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import statsmodels.api as sm

# ─── 1) Utility functions ─────────────────────────────────────────────────────
def get_market_data(ticker, start_date, end_date):
    data = yf.download(ticker, start=start_date, end=end_date, progress=False)
    if data.empty:
        raise ValueError(f"No data for {ticker} between {start_date} and {end_date}")
    return data

def calculate_returns(price_data):
    col = 'Adj Close' if 'Adj Close' in price_data.columns else 'Close'
    returns = price_data[col].pct_change().dropna()
    if isinstance(returns, pd.DataFrame):
        returns = returns.iloc[:, 0]
    return returns

def estimate_leverage_parameters(etf_ret, mkt_ret):
    idx = etf_ret.index.intersection(mkt_ret.index)
    y = etf_ret.loc[idx]
    X = sm.add_constant(mkt_ret.loc[idx])
    model = sm.OLS(y, X).fit()
    alpha, beta = model.params
    print(f"alpha={alpha:.6f}, beta={beta:.6f}, R²={model.rsquared:.4f}")
    return alpha, beta

# ─── 2) Pair-Trading Strategy ───────────────────────────────────────────────────
class ETFPairTradingStrategy:
    def __init__(self, bull_weight, bear_weight, rebalance_threshold, inout_threshold):
        total = bull_weight + bear_weight
        self.bull_weight      = bull_weight / total
        self.bear_weight      = bear_weight / total
        self.rebalance_thresh = rebalance_threshold
        self.inout_thresh     = inout_threshold

    def reset(self, initial_capital):
        self.long_t        = initial_capital
        self.sh_bull       = self.bull_weight * initial_capital
        self.sh_bear       = self.bear_weight * initial_capital
        self.current_value = initial_capital
        return self.current_value

    def step(self, ret_bull, ret_bear, ret_tbill):
        prev = self.current_value
        self.sh_bull *= (1 - ret_bull)
        self.sh_bear *= (1 - ret_bear)
        self.long_t   *= (1 + ret_tbill)

        cost = 0.0
        ts   = self.sh_bull + self.sh_bear

        curr_bw = self.sh_bull/ts if ts>0 else self.bull_weight
        low, high = self.bull_weight-self.rebalance_thresh, self.bull_weight+self.rebalance_thresh
        if curr_bw < low or curr_bw > high:
            traded = 2*abs(self.bull_weight-curr_bw)*ts
            self.sh_bull = self.bull_weight*ts
            self.sh_bear = self.bear_weight*ts
            cost += traded

        ts    = self.sh_bull + self.sh_bear
        ratio = self.long_t/ts if ts>0 else 1.0
        if ratio < (1-self.inout_thresh):
            traded = abs(1-ratio)*ts
            factor = ts/self.long_t
            self.sh_bull /= factor
            self.sh_bear /= factor
            cost += traded
        elif ratio > (1+self.inout_thresh):
            traded = abs(ratio-1)*ts
            factor = self.long_t/ts
            self.sh_bull *= factor
            self.sh_bear *= factor
            cost += traded

        self.current_value = self.long_t + self.sh_bull + self.sh_bear
        reward = (self.current_value - prev) - 0.001*cost
        return self.current_value, reward

# ─── 3) RL environment wrapper ─────────────────────────────────────────────────
try:
    import gymnasium as gym
    from gymnasium import spaces
    USE_GYMNASIUM = True
except ImportError:
    import gym
    from gym import spaces
    USE_GYMNASIUM = False

class PairEnv(gym.Env):
    def __init__(self, returns_df, window=20, initial_capital=10000):
        super().__init__()
        self.returns = returns_df.reset_index(drop=True)
        self.window  = window
        self.initial = initial_capital
        self.n       = len(self.returns)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)
        self.action_space      = spaces.Box(low=np.array([0.0,0.0]), high=np.array([0.2,1.0]), dtype=np.float32)

    def reset(self, *, seed=None, options=None):
        if USE_GYMNASIUM:
            super().reset(seed=seed)
        self.t = self.window
        self.strategy = ETFPairTradingStrategy(1/3, 2/3, 0.05, 0.05)
        self.strategy.reset(self.initial)
        return (self._get_state(), {}) if USE_GYMNASIUM else self._get_state()

    def step(self, action):
        δ, w = action
        self.strategy.rebalance_thresh = float(δ)
        self.strategy.bull_weight      = float(w)
        self.strategy.bear_weight      = 1.0 - float(w)
        r = self.returns.loc[self.t]
        _, reward = self.strategy.step(r["UPRO"], r["SPXU"], r["TBill"])
        self.t += 1
        done = self.t >= self.n
        obs = self._get_state()
        return (obs, reward, done, False, {}) if USE_GYMNASIUM else (obs, reward, done, {})

    def _get_state(self):
        seg = self.returns.iloc[self.t-self.window:self.t]
        spx = seg["^GSPC"]; vix = seg["^VIX"].iloc[-1]
        vol = spx.std(); mom = np.prod(1+spx.values)-1
        bw  = self.strategy.sh_bull/(self.strategy.sh_bull+self.strategy.sh_bear)
        mr  = self.strategy.long_t  /(self.strategy.sh_bull+self.strategy.sh_bear)
        return np.array([vol, vix, mom, bw, mr], dtype=np.float32)

# ─── 4) Main: data, train, backtest, plot ──────────────────────────────────────
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback

def main():
    start, end = '2010-01-01', '2015-12-31'
    data = {t: get_market_data(t, start, end) for t in ["^GSPC","UPRO","SPXU","^VIX"]}

    rets = pd.DataFrame(index=data["^GSPC"].index)
    for t in ["^GSPC","UPRO","SPXU","^VIX"]:
        rets[t] = calculate_returns(data[t])
    rets["TBill"] = (1+0.02)**(1/252)-1
    rets.dropna(inplace=True)

    estimate_leverage_parameters(rets["UPRO"], rets["^GSPC"])
    estimate_leverage_parameters(rets["SPXU"], rets["^GSPC"])

    split = int(len(rets)*0.8)
    train_df, eval_df = rets.iloc[:split], rets.iloc[split:]

    train_env = PairEnv(train_df, window=20)
    eval_env  = PairEnv(eval_df,  window=20)
    eval_cb = EvalCallback(eval_env, best_model_save_path="./logs/",
                           log_path="./logs/", eval_freq=500, n_eval_episodes=3)

    model = PPO("MlpPolicy", train_env, learning_rate=3e-4, batch_size=64, verbose=1)
    model.learn(total_timesteps=20000, callback=eval_cb)
    model.save("ppo-pair-agent")

    static = ETFPairTradingStrategy(1/3, 2/3, rebalance_threshold=0.2, inout_threshold=0.1)
    static_vals = [static.reset(10000)]
    for i in range(20, len(eval_df)):
        row = eval_df.iloc[i]
        v,_ = static.step(row["UPRO"], row["SPXU"], row["TBill"])
        static_vals.append(v)
    static_pv = pd.Series(static_vals[1:], index=eval_df.index[20:])

    model = PPO.load("ppo-pair-agent")
    env = PairEnv(eval_df, window=20)
    obs = env.reset()[0] if USE_GYMNASIUM else env.reset()
    rl_vals = [env.strategy.current_value]
    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, _, done, *_ = env.step(action) if USE_GYMNASIUM else env.step(action)
        rl_vals.append(env.strategy.current_value)
    rl_pv = pd.Series(rl_vals[1:], index=eval_df.index[20:])

    plt.figure(figsize=(12,6))
    plt.plot(static_pv, label="Static Strategy (δ=0.2, θ=0.1)")
    plt.plot(rl_pv,     label="RL-Dynamic Strategy")
    plt.legend(); plt.title("Static vs RL-Dynamic Pair Trading"); plt.grid(True)
    plt.show()

if __name__ == "__main__":
    main()